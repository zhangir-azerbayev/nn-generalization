# Neural Network Generalization Reading List 
Why do neural networks generalize? An actively maintained reading list. 

## Preliminaries 
| Author | Title | Year | Type | Remarks |
|--------|-------|------|------|---------|
|Hardt and Recht| ["Generalization"](https://mlstory.org/generalization.html), in *Patterns, Prediction, and Action*| 2021|book chapter|Classical ideas in statistical learning theory and empirical phenomena in deep learning |
|Friedman, Hastie, and Tibshirani | "Model Assesment and Selection",in *Elements of Statistical Learning* | 2008 |book chapter| A good overview of classical learning theory. The ideas that explain classical ML algorithms, but seem to contradict empirical phenomena in deep learning| 

## Deep Double Descent 
| Author | Title | Year | Type | Remarks |
|--------|-------|------|------|---------|
|Hubinger|[Understanding "Deep Double Descent"](https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent) |2019|blog post| |
|Nakkiran et al.| [Deep Double Descent: Where Bigger Models and More Data Hurt](https://mltheory.org/deep.pdf) | 2019 | paper | The original deep double descent paper| 
|Keskar et al.|[On Large-Batch Training For Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836) | 2017 | paper | | 
|Dinh et al.|[Sharp Minima Can Generalize For Deep Nets](https://arxiv.org/abs/1703.04933)| 2017 | paper | | 
| Neyshabur et al.| [Exploring Generalization in Deep Learning](https://papers.nips.cc/paper/2017/hash/10ce03a1ed01077e3e289f3e53c72813-Abstract.html) | 2017 | paper | Scale normalization, connection between sharpness and PAC-Bayes theory. | 
|Frankle and Carbin | [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) | 2018 | paper | |

## Data Manifolds 
| Author | Title | Year | Type | Remarks |
|--------|-------|------|------|---------|
|Olah | [Neural Networks, Manifolds, and Topology](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) | 2014 | blog post| |Fefferman et al.| [Testing the manifold hypothesis](https://www.ams.org/journals/jams/2016-29-04/S0894-0347-2016-00852-4/) | 2016 | paper | |

## Generalization Bounds

## Optimization Dynamics 

## Scaling Laws and Pre-training

